import torch
from datasets import load_dataset
from transformers import AutoTokenizer
from trl import PPOTrainer, AutoModelForCausalLMWithValueHead, PPOConfig
from peft import LoraConfig
import os
import re
import json
from tqdm import tqdm
import time
from accelerate import Accelerator
import random

# çŽ¯å¢ƒè®¾ç½®
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# å½©è‰²æ—¥å¿—å·¥å…· (æ— ä¿®æ”¹)
class COLORS:
    HEADER = '\033[95m'
    OKGREEN = '\033[92m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

# =====================================================================================
# 1. å¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒ
# =====================================================================================
class EcommerceEnv:
    """
    MODIFIED: This version is now robust to conversations with consecutive
    user or assistant turns by using an index to track progress.
    """
    def __init__(self, scenarios, tokenizer):
        self.scenarios = scenarios
        self.tokenizer = tokenizer
        # è¿™äº›å˜é‡åœ¨ reset() ä¸­è¢«åˆå§‹åŒ–
        self.current_trace = None
        self.history = None
        self.turn_index = 0

    def reset(self):
        """
        éšæœºé€‰æ‹©ä¸€ä¸ªåœºæ™¯ï¼Œå¹¶æ‰¾åˆ°ç¬¬ä¸€ä¸ªassistantå›žåˆçš„ä½ç½®æ¥åˆå§‹åŒ–çŽ¯å¢ƒã€‚
        """
        self.current_trace = random.choice(self.scenarios)
        self.history = []
        
        # å¯»æ‰¾ç¬¬ä¸€ä¸ª assistant å›žåˆçš„ç´¢å¼•
        first_assistant_idx = -1
        for i, msg in enumerate(self.current_trace['messages']):
            if msg['role'] == 'assistant':
                first_assistant_idx = i
                break
        
        # å¦‚æžœæ‰¾ä¸åˆ°assistantå›žåˆï¼ˆæ•°æ®æ ¼å¼é—®é¢˜ï¼‰ï¼Œåˆ™é‡æ–°éšæœºé€‰æ‹©
        if first_assistant_idx == -1:
            return self.reset()

        # åˆå§‹åŒ–åŽ†å²è®°å½•ï¼ŒåŒ…å«ç¬¬ä¸€ä¸ªassistantå›žåˆä¹‹å‰çš„æ‰€æœ‰å†…å®¹
        self.history = self.current_trace['messages'][:first_assistant_idx]
        self.turn_index = first_assistant_idx
        
        return self.history

    def step(self, generated_json_str: str):
        """
        æ ¹æ®æ¨¡åž‹ç”Ÿæˆçš„å†…å®¹æŽ¨è¿›å¯¹è¯ï¼Œå¹¶è®¡ç®—å¥–åŠ±ã€‚
        """
        # èŽ·å–å½“å‰æŒ‡é’ˆä½ç½®çš„â€œé»„é‡‘æ ‡å‡†ç­”æ¡ˆâ€
        golden_assistant_message = self.current_trace['messages'][self.turn_index]
        golden_json_str = golden_assistant_message['content']
        
        # è®¡ç®—å¥–åŠ±
        reward = self.get_reward(generated_json_str, golden_json_str)
        
        # å°†æ¨¡åž‹çš„ç”Ÿæˆç»“æžœï¼ˆå®žé™…è¡Œä¸ºï¼‰åŠ å…¥åŽ†å²
        self.history.append({"role": "assistant", "content": generated_json_str})
        
        # --- æ ¸å¿ƒä¿®æ”¹é€»è¾‘ï¼šæ™ºèƒ½åœ°å¯»æ‰¾ä¸‹ä¸€ä¸ªç”¨æˆ·å›žåˆ ---
        # ä»Žå½“å‰æ ‡å‡†ç­”æ¡ˆä¹‹åŽå¼€å§‹ï¼Œå¯»æ‰¾ä¸‹ä¸€ä¸ªassistantå›žåˆ
        next_assistant_idx = -1
        for i in range(self.turn_index + 1, len(self.current_trace['messages'])):
            message = self.current_trace['messages'][i]
            # æŠŠæ‰€æœ‰é‡åˆ°çš„userå›žåˆéƒ½åŠ å…¥åŽ†å²
            if message['role'] == 'user':
                self.history.append(message)
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªassistantå›žåˆå°±åœæ­¢
            elif message['role'] == 'assistant':
                next_assistant_idx = i
                break
        
        # å¦‚æžœæ‰¾åˆ°äº†ä¸‹ä¸€ä¸ªassistantå›žåˆ
        if next_assistant_idx != -1:
            self.turn_index = next_assistant_idx
            done = False
        else:
            # å¦‚æžœæ²¡æ‰¾åˆ°ï¼Œè¯´æ˜Žå¯¹è¯ç»“æŸ
            done = True
            
        return self.history, reward, done

    def get_reward(self, generated_str: str, golden_str: str):
        """
        å¥–åŠ±å‡½æ•°ï¼Œå¢žåŠ äº†å¯¹é»„é‡‘æ ‡å‡†æ•°æ®æ ¼å¼çš„å¥å£®æ€§æ£€æŸ¥ã€‚
        """
        try:
            golden_actions = json.loads(golden_str)
            if not isinstance(golden_actions, list):
                golden_actions = [golden_actions]
        except (json.JSONDecodeError, TypeError):
             # è¿™æ˜¯ä¸€ä¸ªä¿æŠ¤æŽªæ–½ï¼Œå¦‚æžœé»„é‡‘æ•°æ®æœ¬èº«æœ‰é—®é¢˜ï¼Œå°±è·³è¿‡è¿™ä¸ªæ ·æœ¬
            print(f"Warning: Skipping a scenario due to malformed golden_str: {golden_str}")
            return 0.0, True # è¿”å›ž0å¥–åŠ±å¹¶ç»“æŸ

        try:
            gen_actions = json.loads(generated_str)
            if not isinstance(gen_actions, list):
                gen_actions = [gen_actions]
        except json.JSONDecodeError:
            return -2.0

        if gen_actions == golden_actions:
            return 2.0

        gen_tool_names = {action.get('tool_name') for action in gen_actions}
        golden_tool_names = {action.get('tool_name') for action in golden_actions}

        if gen_tool_names != golden_tool_names:
            return -2.0
        
        return -1.0

# =====================================================================================
# 2. ä¸»è®­ç»ƒæµç¨‹ 
# =====================================================================================
if __name__ == "__main__":
    accelerator = Accelerator()

    sft_model_path = "./qwen2-1.5b-sft-lora-adapters-bf16" # SFTæ¨¡åž‹è·¯å¾„
    rl_adapter_path = "./qwen2-1.5b-rl-adapters" # æ–°çš„RLæ¨¡åž‹ä¿å­˜è·¯å¾„
    dataset_path = "./sft_data.jsonl" # SFTæ—¶ç”¨çš„æ•°æ®é›†

    config = PPOConfig(
        model_name=sft_model_path,
        learning_rate=1.41e-5,
        batch_size=16,
        mini_batch_size=2,
        # SUGGESTION 3: ç®€åŒ–é…ç½®ï¼Œåœ¨PPOä¸­é€šå¸¸ä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
        gradient_accumulation_steps=1,
        log_with="tensorboard",
        project_kwargs={"logging_dir": "./rl_logs_v2"},
        kl_penalty="kl",
        adap_kl_ctrl=True,
        init_kl_coef=0.1,
        ppo_epochs=4,
    )

    tokenizer = AutoTokenizer.from_pretrained(sft_model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    # å¯¹äºŽç”Ÿæˆä»»åŠ¡ï¼Œpaddingåœ¨å·¦è¾¹æ˜¯æœ€ä½³å®žè·µ
    tokenizer.padding_side = "left"

    lora_config = LoraConfig(
        r=16, lora_alpha=32, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj"]
    )

    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        sft_model_path,
        trust_remote_code=True,
        peft_config=lora_config,
        torch_dtype=torch.bfloat16,
    )
    
    
    ppo_trainer = PPOTrainer(config=config, model=model, tokenizer=tokenizer)
    
    scenarios = list(load_dataset("json", data_files=dataset_path, split="train"))
    env = EcommerceEnv(scenarios, tokenizer)

    generation_kwargs = {
        "min_length": -1, "top_k": 0.0, "top_p": 1.0, "do_sample": True,
        "pad_token_id": tokenizer.eos_token_id, "max_new_tokens": 256,
    }

    total_ppo_epochs = 4
    
    if accelerator.is_main_process:
        print(f"\n{COLORS.HEADER}===== ðŸš€ Starting PPO Training ====={COLORS.ENDC}")
        
    for epoch in range(total_ppo_epochs):
        if accelerator.is_main_process:
            print(f"\n{COLORS.BOLD}Epoch {epoch+1}/{total_ppo_epochs}{COLORS.ENDC}")
            scenario_iterator = tqdm(scenarios, desc="Scenario")
        else:
            scenario_iterator = scenarios

        batch_queries, batch_responses, batch_rewards = [], [], []
        
        for scenario in scenario_iterator:
            history = env.reset()
            done = False
            
            while not done:
                query_text = tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)
                # SUGGESTION 3: PPOTrainerå†…éƒ¨ä¼šå¤„ç†è®¾å¤‡ï¼Œæ— éœ€æ‰‹åŠ¨.to(device)
                query_tensor = tokenizer.encode(query_text, return_tensors="pt").squeeze(0).to(accelerator.device)

                # ç”Ÿæˆã€è§£ç ã€è®¡ç®—å¥–åŠ±
                response_tensor = ppo_trainer.generate(query_tensor, **generation_kwargs)
                response_only_tensor = response_tensor.squeeze()[len(query_tensor):]
                # ä»Žå®Œæ•´è¾“å‡ºä¸­æå–æ¨¡åž‹æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                action_text = tokenizer.decode(response_only_tensor, skip_special_tokens=True)
                
                history, reward_float, done = env.step(action_text.strip())
                
                # ä¿å­˜æ•°æ®ï¼Œå‡†å¤‡PPOæ­¥éª¤
                batch_queries.append(query_tensor)
                batch_responses.append(response_tensor.squeeze())
                batch_rewards.append(torch.tensor(reward_float, device=accelerator.device))

                # å½“æ”¶é›†åˆ°è¶³å¤Ÿæ•°æ®æ—¶ï¼Œæ‰§è¡ŒPPOä¼˜åŒ–æ­¥éª¤
                if len(batch_queries) >= config.batch_size:
                    stats = ppo_trainer.step(batch_queries, batch_responses, batch_rewards)
                    ppo_trainer.log_stats(stats, {}, [r.cpu().item() for r in batch_rewards])
                    batch_queries, batch_responses, batch_rewards = [], [], []

    if accelerator.is_main_process:
        print(f"\n{COLORS.HEADER}===== âœ… RL Training Complete. Saving Adapters... ====={COLORS.ENDC}")
        ppo_trainer.save_pretrained(rl_adapter_path)
        print(f"{COLORS.OKGREEN}âœ… Adapters saved to: {rl_adapter_path}{COLORS.ENDC}")