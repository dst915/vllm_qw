import torch
from datasets import load_dataset
from transformers import AutoTokenizer
from trl import PPOTrainer, AutoModelForCausalLMWithValueHead, PPOConfig
from peft import LoraConfig
import os
import re
import json
from tqdm import tqdm
import time
from accelerate import Accelerator
import random

# çŽ¯å¢ƒè®¾ç½®
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# å½©è‰²æ—¥å¿—å·¥å…· (æ— ä¿®æ”¹)
class COLORS:
    HEADER = '\033[95m'
    OKGREEN = '\033[92m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

# =====================================================================================
# 1. å¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒ
# =====================================================================================
class EcommerceEnv:
    def __init__(self, scenarios, tokenizer):
        self.scenarios = scenarios
        self.tokenizer = tokenizer
        self.system_prompt = self._build_system_prompt()
        self.current_trace = None
        self.history = []

    def _build_system_prompt(self):
        # ä¿æŒå’ŒSFTé˜¶æ®µå®Œå…¨ä¸€è‡´çš„ç³»ç»ŸæŒ‡ä»¤
        return """You are an intelligent e-commerce assistant... (çœç•¥ï¼Œä½¿ç”¨ä½ SFTæ—¶çš„å®Œæ•´prompt)"""

    def reset(self):
        # éšæœºé€‰æ‹©ä¸€ä¸ªå¤šè½®å¯¹è¯åœºæ™¯
        self.current_trace = random.choice(self.scenarios)
        # åˆå§‹åŒ–åŽ†å²è®°å½•ï¼ŒåŒ…å«ç³»ç»ŸæŒ‡ä»¤å’Œç¬¬ä¸€è½®çš„ç”¨æˆ·è¾“å…¥
        self.history = self.current_trace['messages'][:2] 
        return self.history

    def step(self, generated_json_str: str):
        # å¦‚æžœåŽ†å²è®°å½•é•¿åº¦å·²ç»è¶…è¿‡äº†æ ‡å‡†ç­”æ¡ˆçš„é•¿åº¦ï¼Œè¯´æ˜Žæ¨¡åž‹ç”Ÿæˆäº†å¤šä½™çš„å¯¹è¯ï¼Œç»™äºˆæƒ©ç½š
        if len(self.history) >= len(self.current_trace['messages']):
            return self.history, -2.0, True

        # èŽ·å–å½“å‰è½®æ¬¡çš„æ ‡å‡†ç­”æ¡ˆ
        golden_assistant_message = self.current_trace['messages'][len(self.history)]
        golden_json_str = golden_assistant_message['content']
        
        # è®¡ç®—å¥–åŠ±
        reward = self.get_reward(generated_json_str, golden_json_str)
        
        # å°†æ¨¡åž‹çš„ç”Ÿæˆç»“æžœåŠ å…¥åŽ†å²
        self.history.append({"role": "assistant", "content": generated_json_str})
        
        # åˆ¤æ–­å¯¹è¯æ˜¯å¦ç»“æŸ
        done = len(self.history) >= len(self.current_trace['messages'])
        
        if not done:
            # å¦‚æžœæ²¡ç»“æŸï¼Œè‡ªåŠ¨åŠ å…¥ä¸‹ä¸€è½®ç”¨æˆ·çš„å‘è¨€
            self.history.append(self.current_trace['messages'][len(self.history)])
            
        return self.history, reward, done

    # CRITICAL FIX 1: é‡å†™å¥–åŠ±å‡½æ•°ä»¥è§£æžå’Œè¯„ä¼°JSON
# åœ¨ EcommerceEnv ç±»ä¸­ï¼Œæ›¿æ¢è¿™ä¸ªå‡½æ•°

    def get_reward(self, generated_str: str, golden_str: str):
        """
        MODIFIED: Added a robust try-except block to catch and report errors
        in the golden dataset, which helps in debugging the data itself.
        """
        # --- æ ¸å¿ƒä¿®æ”¹ç‚¹ï¼šä¸ºgolden_strçš„è§£æžæ·»åŠ ä¿æŠ¤ ---
        try:
            golden_actions = json.loads(golden_str)
            if not isinstance(golden_actions, list):
                golden_actions = [golden_actions]
        except json.JSONDecodeError:
            print(f"\n{COLORS.FAIL}CRITICAL ERROR: Failed to parse golden_str as JSON.{COLORS.ENDC}")
            # å°è¯•ä»Ž self.current_trace ä¸­èŽ·å– session_id æ¥å¸®åŠ©å®šä½
            # æ³¨æ„ï¼š'messages' é”®å­˜åœ¨äºŽè½¬æ¢åŽçš„æ•°æ®ä¸­ï¼Œä½†åŽŸå§‹çš„ session_id å¯èƒ½éœ€è¦ä»ŽåŽŸå§‹æ•°æ®ç»“æž„ä¸­å¯»æ‰¾
            # ä¸ºç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æŽ¥æ‰“å°å‡ºå¯¼è‡´é—®é¢˜çš„å­—ç¬¦ä¸²
            print(f"{COLORS.FAIL}Problematic golden_str: -->{golden_str}<--{COLORS.ENDC}")
            print(f"{COLORS.FAIL}This indicates a problem in your sft_data_transformed.jsonl file.{COLORS.ENDC}")
            # æŠ›å‡ºå¼‚å¸¸ï¼Œä¸­æ–­ç¨‹åºï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªéœ€è¦ä¿®å¤çš„æ•°æ®é—®é¢˜
            raise
        # --- ä¿®æ”¹ç»“æŸ ---

        try:
            gen_actions = json.loads(generated_str)
            if not isinstance(gen_actions, list):
                gen_actions = [gen_actions]
        except json.JSONDecodeError:
            return -2.0 # æ¨¡åž‹ç”Ÿæˆäº†æ— æ•ˆJSONï¼Œç»™äºˆæƒ©ç½š

        # å¥–åŠ±é€»è¾‘ä¿æŒä¸å˜
        if gen_actions == golden_actions:
            return 2.0

        gen_tool_names = {action.get('tool_name') for action in gen_actions}
        golden_tool_names = {action.get('tool_name') for action in golden_actions}

        if gen_tool_names != golden_tool_names:
            return -2.0
        
        return -1.0

# =====================================================================================
# 2. ä¸»è®­ç»ƒæµç¨‹ 
# =====================================================================================
if __name__ == "__main__":
    accelerator = Accelerator()

    sft_model_path = "./qwen2-1.5b-sft-lora-adapters-bf16" # SFTæ¨¡åž‹è·¯å¾„
    rl_adapter_path = "./qwen2-1.5b-rl-adapters" # æ–°çš„RLæ¨¡åž‹ä¿å­˜è·¯å¾„
    dataset_path = "./sft_data.jsonl" # SFTæ—¶ç”¨çš„æ•°æ®é›†

    config = PPOConfig(
        model_name=sft_model_path,
        learning_rate=1.41e-5,
        batch_size=16,
        mini_batch_size=2,
        # SUGGESTION 3: ç®€åŒ–é…ç½®ï¼Œåœ¨PPOä¸­é€šå¸¸ä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
        gradient_accumulation_steps=1,
        log_with="tensorboard",
        project_kwargs={"logging_dir": "./rl_logs_v2"},
        kl_penalty="kl",
        adap_kl_ctrl=True,
        init_kl_coef=0.1,
        ppo_epochs=4,
    )

    tokenizer = AutoTokenizer.from_pretrained(sft_model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    # å¯¹äºŽç”Ÿæˆä»»åŠ¡ï¼Œpaddingåœ¨å·¦è¾¹æ˜¯æœ€ä½³å®žè·µ
    tokenizer.padding_side = "left"

    lora_config = LoraConfig(
        r=16, lora_alpha=32, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj"]
    )

    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        sft_model_path,
        trust_remote_code=True,
        peft_config=lora_config,
        torch_dtype=torch.bfloat16,
    )
    
    
    ppo_trainer = PPOTrainer(config=config, model=model, tokenizer=tokenizer)
    
    scenarios = list(load_dataset("json", data_files=dataset_path, split="train"))
    env = EcommerceEnv(scenarios, tokenizer)

    generation_kwargs = {
        "min_length": -1, "top_k": 0.0, "top_p": 1.0, "do_sample": True,
        "pad_token_id": tokenizer.eos_token_id, "max_new_tokens": 256,
    }

    total_ppo_epochs = 4
    
    if accelerator.is_main_process:
        print(f"\n{COLORS.HEADER}===== ðŸš€ Starting PPO Training ====={COLORS.ENDC}")
        
    for epoch in range(total_ppo_epochs):
        if accelerator.is_main_process:
            print(f"\n{COLORS.BOLD}Epoch {epoch+1}/{total_ppo_epochs}{COLORS.ENDC}")
            scenario_iterator = tqdm(scenarios, desc="Scenario")
        else:
            scenario_iterator = scenarios

        batch_queries, batch_responses, batch_rewards = [], [], []
        
        for scenario in scenario_iterator:
            history = env.reset()
            done = False
            
            while not done:
                query_text = tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)
                # SUGGESTION 3: PPOTrainerå†…éƒ¨ä¼šå¤„ç†è®¾å¤‡ï¼Œæ— éœ€æ‰‹åŠ¨.to(device)
                query_tensor = tokenizer.encode(query_text, return_tensors="pt").squeeze(0).to(accelerator.device)

                # ç”Ÿæˆã€è§£ç ã€è®¡ç®—å¥–åŠ±
                response_tensor = ppo_trainer.generate(query_tensor, **generation_kwargs)
                response_only_tensor = response_tensor.squeeze()[len(query_tensor):]
                # ä»Žå®Œæ•´è¾“å‡ºä¸­æå–æ¨¡åž‹æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                action_text = tokenizer.decode(response_only_tensor, skip_special_tokens=True)
                
                history, reward_float, done = env.step(action_text.strip())
                
                # ä¿å­˜æ•°æ®ï¼Œå‡†å¤‡PPOæ­¥éª¤
                batch_queries.append(query_tensor)
                batch_responses.append(response_tensor.squeeze())
                batch_rewards.append(torch.tensor(reward_float, device=accelerator.device))

                # å½“æ”¶é›†åˆ°è¶³å¤Ÿæ•°æ®æ—¶ï¼Œæ‰§è¡ŒPPOä¼˜åŒ–æ­¥éª¤
                if len(batch_queries) >= config.batch_size:
                    stats = ppo_trainer.step(batch_queries, batch_responses, batch_rewards)
                    ppo_trainer.log_stats(stats, {}, [r.cpu().item() for r in batch_rewards])
                    batch_queries, batch_responses, batch_rewards = [], [], []

    if accelerator.is_main_process:
        print(f"\n{COLORS.HEADER}===== âœ… RL Training Complete. Saving Adapters... ====={COLORS.ENDC}")
        ppo_trainer.save_pretrained(rl_adapter_path)
        print(f"{COLORS.OKGREEN}âœ… Adapters saved to: {rl_adapter_path}{COLORS.ENDC}")